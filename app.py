import streamlit as st
import requests
from datetime import datetime
import json
import pandas as pd

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ìƒìˆ˜ ì •ì˜
MAX_MESSAGES = 50
API_TIMEOUT = 30
API_URL = "https://model.odyssey-ai.svc.cluster.local/v1/completions"

# ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
    <style>
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        margin-left: 20%;
    }
    .assistant-message {
        background-color: #f5f5f5;
        margin-right: 20%;
    }
    .message-timestamp {
        color: #666;
        font-size: 0.8rem;
    }
    </style>
    """, unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state['messages'] = []
if 'processing' not in st.session_state:
    st.session_state['processing'] = False

def call_llm_api(prompt):
    """LLM API í˜¸ì¶œ"""
    try:
        response = requests.post(
            API_URL,
            json={
                "model": "model",
                "prompt": prompt,
                "max_tokens": 256,        # ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                "temperature": 0.3,       # ë‚®ì€ ë¬´ì‘ìœ„ì„±ìœ¼ë¡œ ì¼ê´€ëœ ì‘ë‹µ
                "top_p": 0.9,            # ë†’ì€ í™•ë¥ ì˜ í† í°ë§Œ ì„ íƒ
                "presence_penalty": 0.1,  # ì•½ê°„ì˜ ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€
                "frequency_penalty": 0.3, # ë‹¨ì–´ ë°˜ë³µ ê°ì†Œ
                "stop": ["\n\n"],        # ëª…í™•í•œ ì‘ë‹µ êµ¬ë¶„
                "n": 1,
            },
            verify=False,
            timeout=API_TIMEOUT
        )
        return response.json()["choices"][0]["text"].strip()
    except Exception as e:
        return f"Error: {str(e)}"

# ë©”ì¸ í™”ë©´
st.title("AI ì±—ë´‡")

# ì±„íŒ… ì´ë ¥ í‘œì‹œ
for message in st.session_state['messages']:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        st.caption(f"{message['timestamp']}")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    if not st.session_state['processing']:  # ì²˜ë¦¬ ì¤‘ì´ ì•„ë‹ ë•Œë§Œ ì‹¤í–‰
        st.session_state['processing'] = True  # ì²˜ë¦¬ ì‹œì‘
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        with st.chat_message("user"):
            st.markdown(prompt)
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            st.caption(timestamp)
        
        st.session_state['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": timestamp
        })

        # AI ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                response = call_llm_api(prompt)
                st.markdown(response)
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                st.caption(timestamp)
        
        st.session_state['messages'].append({
            "role": "assistant",
            "content": response,
            "timestamp": timestamp
        })
        
        # ë©”ì‹œì§€ ìˆ˜ ì œí•œ
        if len(st.session_state['messages']) > MAX_MESSAGES:
            st.session_state['messages'] = st.session_state['messages'][-MAX_MESSAGES:]
        
        st.session_state['processing'] = False  # ì²˜ë¦¬ ì™„ë£Œ

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("ì„¤ì •")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state['messages'] = []
        st.rerun()
    
    # ëŒ€í™” ë‚´ë³´ë‚´ê¸°
    if st.session_state['messages']:
        df = pd.DataFrame(st.session_state['messages'])
        csv = df.to_csv(index=False)
        st.download_button(
            label="ëŒ€í™” ë‚´ì—­ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=f"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
